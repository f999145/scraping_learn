# scraping_learn

Скрипт действий:
    1. Определить с помощью какого метода можно собрать информацию с сайта.
       1. С помощью `requests`
          1. В данном случае будут задействованна много поточность.
          2. Можно использовать юпитер. 
          3. Если получаем код ошибки, необходимо добавить заголовки, кукис, и параметры.([curlconverter](https://curlconverter.com/))
       2. В случае если метод requests не работает, поможет селениум или undetected_chromedriver
          1. Селениум можно запустить как в многопотоке так и в мультироцесорности.
          2. Селениум так же можно запустить скрытно.
          3. В случае если действует защита, типа Cloudflare, поможет undetected_chromedriver
    2. Собранную информацию следует сохранить, для последующего парсинга данных.
    3. В любом случае Парсинг сайтов следует проводить отдельно от сбора информации.
       1. Покрайней мере так следует делать в случаях большого объема данных.
       2. Это позволит исследовать на возможные ошибки в данных.
       3. Сильно ускорить процесс парсинга если использовать многопроцессорность.
       4. Многопроцессорный скрипт можно запустить в Юпитере, а также получить данные из него.
    4. Структурировать данные и сохранить их.
    5. Собрать на основе текущих данных, парсер в реальном времени, который не будет обрабатывать большое количество данных.