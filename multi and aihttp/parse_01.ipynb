{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile, ZIP_DEFLATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_url = '45.81.76.252'\n",
    "proxy_port = '8000'\n",
    "\n",
    "proxy = {\n",
    "    'https': f'http://{proxy_url}:{proxy_port}'\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/111.0',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'searchString': '',\n",
    "    'morphology': 'on',\n",
    "    'search-filter': 'Дате обновления',\n",
    "    'savedSearchSettingsIdHidden': '',\n",
    "    'fz94': 'on',\n",
    "    'fz223': 'on',\n",
    "    'published': 'on',\n",
    "    'regarded': 'on',\n",
    "    'considered': 'on',\n",
    "    'decisionOnTheComplaintTypeResult': '',\n",
    "    'receiptDateStart': '',\n",
    "    'receiptDateEnd': '',\n",
    "    'updateDateFrom': '',\n",
    "    'updateDateTo': '',\n",
    "    'sortBy': 'UPDATE_DATE',\n",
    "    'pageNumber': '',\n",
    "    'sortDirection': 'false',\n",
    "    'recordsPerPage': '_10',\n",
    "    'showLotsInfoHidden': 'false',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"\"\"\n",
    "\n",
    "https://zakupki.gov.ru/epz/complaint/search/search_eis.html\n",
    "\n",
    "\"\"\".strip()\n",
    "host = '/'.join(url.split('/', maxsplit=3)[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_links_in_page(text: str) -> list[str]:\n",
    "    # Получам ссылки с каждой страницы (всего 10)\n",
    "    out_list = list()\n",
    "    soup = BS(text, 'html.parser')\n",
    "    blocks_list = soup.find('div', class_ = 'search-registry-entrys-block')\n",
    "    blocks_list = blocks_list.find_all('div', class_ = 'search-registry-entry-block')\n",
    "\n",
    "    for block in blocks_list:\n",
    "        href = block.find('div', class_ = 'd-flex registry-entry__header-mid align-items-center')\n",
    "        href = host + href.find('span').find('a').get('href')\n",
    "        out_list.append(href)\n",
    "    \n",
    "    return out_list\n",
    "\n",
    "def _parse_links_in_site() -> list[str]:\n",
    "    # Проходим по страницам поиска из реестра жалоб\n",
    "    # всего страниц 100\n",
    "    # на каждой странице 10 жалоб\n",
    "    # на выходе получаем список ссылок на страницу с жалобой\n",
    "    out_list = list()\n",
    "    \n",
    "    with tqdm(total=100 ) as pbar:\n",
    "        for page in range(1,101):\n",
    "            params['pageNumber'] = f'{page}'\n",
    "            response = requests.get(url=url,params=params , headers=headers, proxies=proxy)\n",
    "            out_list.append(_parse_links_in_page(response.text))\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return sum(out_list, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:06<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "links_list = _parse_links_in_site()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение линков в json формате\n",
    "#--------------------------------------------\n",
    "# with open('data/links_list.json', 'w', encoding='utf-8') as file:\n",
    "#     tmp = json.dumps(links_list, indent=4, ensure_ascii=False)\n",
    "#     file.write(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка линков из json\n",
    "#--------------------------------------------\n",
    "# with open('data/links_list.json', 'r', encoding='utf-8') as file:\n",
    "#     tmp = file.read()\n",
    "#     links_list = json.loads(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://zakupki.gov.ru/epz/complaint/card/complaint-information.html?id=2258395'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# number the complaint - Номер жалобы\n",
    "# date the complaint - Дата размещения жалобы\n",
    "\n",
    "# Participant data - Данные участника -----------------\n",
    "# Type of participant - Тип участника\n",
    "# Name of participant - Имя участника\n",
    "# INN of participant - Имя участника\n",
    "# Date of registration - Дата постановки на учет в налоговом органе\n",
    "# Location of participant - Место нахождения\n",
    "# Phone of participant - телефон участника\n",
    "# email - Адрес электронной почты\n",
    "\n",
    "# Information about the subject of control - Информация о субъекте контроля ------------\n",
    "# Subject of control - Субъект контроля\n",
    "# Name of the organization - Наименование организации\n",
    "# Postal address - Почтовый адрес\n",
    "# Location of the subject - Место нахождения\n",
    "# Phone of the subject - Номер телефона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_info_complaint(text: str):\n",
    "    soup = BS(text, 'html.parser')\n",
    "    complaint = soup.find('div', class_='sectionMainInfo borderRight col-9')\n",
    "\n",
    "    number_complaint = complaint.find('div', class_='cardMainInfo__status').find('a').get_text()[2:]\n",
    "\n",
    "    status_complaint = complaint.find('div', class_='cardMainInfo__status').find('span', class_ = 'cardMainInfo__state').get_text()\n",
    "\n",
    "    comments_complaint = list()\n",
    "    try:\n",
    "        tmp = complaint.find('span', class_='cardMainInfo__title distancedText float-left').find('span').get_text().strip()\n",
    "    except AttributeError:\n",
    "        tmp = ''\n",
    "    comments_complaint.append(tmp)\n",
    "    try:\n",
    "        tmp = complaint.find('span', class_='cardMainInfo__title distancedText float-left').find_next_sibling('span', class_='cardMainInfo__title').get_text().strip()\n",
    "    except AttributeError:\n",
    "        tmp = ''\n",
    "    comments_complaint.append(tmp)\n",
    "\n",
    "    card_common = soup.find('div', class_='card-common')\n",
    "    date_complaint = card_common.find(string='Дата размещения жалобы').find_parent().find_next_sibling('div', class_=\"common-text__value\").get_text().strip()\n",
    "    print(number_complaint)\n",
    "\n",
    "\n",
    "def _get_number(text: str) -> str:\n",
    "    soup = BS(text, 'html.parser')\n",
    "    complaint = soup.find('div', class_='sectionMainInfo borderRight col-9')\n",
    "    number_complaint = complaint.find('div', class_='cardMainInfo__status').find('a').get_text()[2:]\n",
    "    return number_complaint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for url in links_list[:10]:\n",
    "#     response = requests.get(url=url, headers=headers, proxies=proxy)\n",
    "#     _parse_info_complaint(response.text)\n",
    "\n",
    "\n",
    "def load_pages(links_list: list[str]) -> dict[str, str]:\n",
    "    with tqdm(total=len(links_list)) as qbar:\n",
    "        out_dict = dict()\n",
    "        for link in links_list:\n",
    "            response = requests.get(url=link, headers=headers, proxies=proxy)\n",
    "            number = _get_number(response.text)\n",
    "            out_dict[number] = response.text\n",
    "            qbar.update(1)\n",
    "        return out_dict\n",
    "\n",
    "def save_pages(in_dict: dict[str, str]) -> None:\n",
    "    with ZipFile('data/pages.zip', 'w', compression=ZIP_DEFLATED, compresslevel=1) as zip_file:\n",
    "        for name, value in in_dict.items():\n",
    "            zip_file.writestr(f'{name}.html', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [08:30<00:00,  1.96it/s]\n"
     ]
    }
   ],
   "source": [
    "pages_dict = load_pages(links_list=links_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pages(pages_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
